# Transformer Implementation from Scratch

A PyTorch implementation of the Transformer architecture as described in ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762) paper. This implementation focuses on clarity and educational value.

## ğŸš€ Features

- Complete transformer architecture implementation
- Multi-head self-attention mechanism
- Positional encoding
- Layer normalization
- Feed-forward networks
- Encoder and decoder stacks
- Masked attention for autoregressive decoding

## ğŸ“¦ Installation

```bash
git clone https://github.com/yourusername/transformer-implementation.git
cd transformer-implementation
pip install -r requirements.txt
```

## ğŸ”§ Requirements

- Python 3.8+
- PyTorch 2.0+
- NumPy
- tqdm

## ğŸ—ï¸ Architecture

The implementation includes the following components:

```
transformer/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ attention.py       # Multi-head attention implementation
â”‚   â”œâ”€â”€ encoder.py         # Transformer encoder
â”‚   â”œâ”€â”€ decoder.py         # Transformer decoder
â”‚   â”œâ”€â”€ positional.py      # Positional encoding
â”‚   â”œâ”€â”€ feed_forward.py    # Feed-forward network
â”‚   â””â”€â”€ transformer.py     # Complete transformer model
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ masking.py        # Attention masking utilities
â”‚   â””â”€â”€ preprocessing.py   # Data preprocessing tools
â”œâ”€â”€ train.py              # Training script
â””â”€â”€ inference.py          # Inference utilities
```

## ğŸ¯ Usage

### Training

```python
from transformer.model import Transformer

# Initialize model
model = Transformer(
    src_vocab_size=32000,
    tgt_vocab_size=32000,
    d_model=512,
    n_heads=8,
    n_layers=6,
    d_ff=2048,
    dropout=0.1
)

# Training example
outputs = model(source_sequences, target_sequences)
```

### Inference

```python
# Generate sequence
generated = model.generate(
    source_sequence,
    max_length=50,
    temperature=0.7
)
```

## ğŸ“ Mathematical Foundations

### Self-Attention

The core self-attention mechanism computes attention scores using queries (Q), keys (K), and values (V):

$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$

where:
- Q âˆˆ â„^(seq_len Ã— d_k): Query matrix
- K âˆˆ â„^(seq_len Ã— d_k): Key matrix
- V âˆˆ â„^(seq_len Ã— d_v): Value matrix
- d_k: Dimension of key vectors
- seq_len: Sequence length

### Multi-Head Attention

Multi-head attention performs h parallel attention operations:

$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$

where each head is computed as:

$head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)$

Matrix dimensions:
- W^Q_i âˆˆ â„^(d_model Ã— d_k)
- W^K_i âˆˆ â„^(d_model Ã— d_k)
- W^V_i âˆˆ â„^(d_model Ã— d_v)
- W^O âˆˆ â„^(hd_v Ã— d_model)

### Positional Encoding
Position is encoded using sine and cosine functions:

$PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})$
$PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})$

where:
- pos: Position in sequence
- i: Dimension index
- d_model: Model dimension

### Masked Attention

Masked attention for decoder self-attention:

$Attention(Q, K, V, M) = softmax(\frac{QK^T}{\sqrt{d_k}} + M)V$

where M is the mask matrix:
```python
M[i,j] = -inf if i < j else 0
```

### Feed-Forward Networks

Each FFN layer applies two linear transformations:

$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$

Matrix dimensions:
- Wâ‚ âˆˆ â„^(d_model Ã— d_ff)
- Wâ‚‚ âˆˆ â„^(d_ff Ã— d_model)
- bâ‚ âˆˆ â„^d_ff
- bâ‚‚ âˆˆ â„^d_model

## ğŸ” Component Details

### Encoder Block

Each encoder layer consists of:
1. Multi-head self-attention
2. Layer normalization
3. Feed-forward network
4. Residual connections

```python
out = LayerNorm(x + MultiHeadAttention(x))
out = LayerNorm(out + FeedForward(out))
```

### Decoder Block

Each decoder layer consists of:
1. Masked multi-head self-attention
2. Multi-head cross-attention with encoder outputs
3. Feed-forward network
4. Layer normalization and residual connections

```python
out = LayerNorm(x + MaskedMultiHeadAttention(x))
out = LayerNorm(out + MultiHeadAttention(out, enc_out))
out = LayerNorm(out + FeedForward(out))
```

### Layer Normalization

Applied after each sub-layer:

$LayerNorm(x) = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$

where:
- Î¼: Mean of the input
- Ïƒ: Standard deviation of the input
- Î³, Î²: Learned parameters
- Îµ: Small constant for numerical stability

### Training Objectives

1. **Cross-Entropy Loss**
   For sequence prediction:
   $L = -\sum_{t=1}^T \sum_{v=1}^V y_{t,v} \log(p_{t,v})$
   where:
   - T: Sequence length
   - V: Vocabulary size
   - y: True labels
   - p: Predicted probabilities

2. **Label Smoothing**
   Applied to target distributions:
   $y'_{t,v} = (1-\alpha)y_{t,v} + \alpha/V$
   where Î± is the smoothing parameter (typically 0.1)

## ğŸ” Implementation Details

Key components are implemented as follows:

1. **Multi-Head Attention**
   - Scaled dot-product attention
   - Parallel attention heads
   - Linear projections for Q, K, V

2. **Position-wise Feed-Forward**
   - Two linear transformations
   - ReLU activation
   - Dropout regularization

3. **Positional Encoding**
   - Sinusoidal position embeddings
   - Learned position embeddings (optional)

## ğŸ“Š Performance

Model performance on standard benchmarks:

- WMT14 EN-DE: 27.5 BLEU
- WMT14 EN-FR: 39.2 BLEU
